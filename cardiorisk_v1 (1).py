# -*- coding: utf-8 -*-
"""CardioRisk V1

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VAG_9Uw4E8NZmOSWmtW6dOM326KoCTGF
"""

import pandas as pd
import kagglehub

UCI = kagglehub.dataset_download('redwankarimsony/heart-disease-data')
Kaggle = kagglehub.dataset_download('johnsmith88/heart-disease-dataset')

df2 = pd.read_csv(Kaggle+"/heart.csv")
df = pd.read_csv(UCI+"/heart_disease_uci.csv")




print('Data source import complete.')

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, QuantileTransformer
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, RandomForestRegressor
from sklearn.svm import SVC
from xgboost import XGBClassifier
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, accuracy_score, classification_report, confusion_matrix, f1_score, precision_score, recall_score, roc_auc_score, roc_curve, auc
from sklearn.tree import plot_tree
from sklearn.model_selection import cross_val_score
from sklearn.pipeline import Pipeline
import random
import warnings
warnings.filterwarnings('ignore')
warnings.filterwarnings("ignore", category=UserWarning)
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", message="No further splits with positive gain")

df.head()

unique_values = df['restecg'].unique()
print(unique_values)

df2.head()

df.info()

df2.info()

df2['sex'] = df2['sex'].replace({1: 'Male', 0: 'Female'})
df2['cp'] = df2['cp'].replace({0: 'asymptomatic', 1: 'atypical angina', 2: 'non-anginal', 3: 'typical angina'})
df2['fbs'] = df2['fbs'].replace({0: 'false', 1: 'true'})
df2['restecg'] = df2['restecg'].replace({0: 'normal', 1: 'ST-T wave abnormality', 2: 'left ventricular hypertrophy'})
df2['exang'] = df2['exang'].replace({0: 'no', 1: 'yes'})
df2['slope'] = df2['slope'].replace({0: 'downsloping', 1: 'flat', 2: 'upsloping'})
df2['thal'] = df2['thal'].replace({1: 'fixed defect', 2: 'normal', 3: 'reversable defect'})
df2['target'] = df2['target'].replace(1,2)
df2['sex'] = df2['sex'].astype(str)
df2['cp'] = df2['cp'].astype(str)
df2['trestbps'] = df2['trestbps'].astype(float)
df2['chol'] = df2['chol'].astype(float)
df2['fbs'] = df2['fbs'].astype(str)
df2['restecg']= df2['restecg'].astype(str)
df2['thalach'] = df2['thalach'].astype(float)
df2['exang'] = df2['exang'].astype(str)
df2['oldpeak'] = df2['oldpeak'].astype(float)
df2['slope'] = df2['slope'].astype(str)
df2['ca'] = df2['ca'].astype(float)
df2['thal'] = df2['thal'].astype(str)

df2.info()

df2.head()

df = df.drop('id', axis=1)
df = df.drop('dataset', axis=1)
df.info()

df2 = df2.rename(columns={'target': 'num'})
df2 = df2.rename(columns={'thalach': 'thalch'})

df = pd.concat([df, df2], axis=0)
df['trestbps'] = pd.to_numeric(df['trestbps'], errors='coerce')

df.head()

df.shape

df['age'].min(), df['age'].max()

sns.histplot(df['num'], kde=True)

sns.histplot(df['age'], kde=True)

sns.histplot(df['age'], kde=True)
plt.axvline(df['age'].mean(), color='red')
plt.axvline(df['age'].median(), color='green')
plt.axvline(df['age'].mode()[0], color='blue')
print("Mean of age column: ", df['age'].mean())
print("Median of age column: ", df['age'].median())
print("Mode of age column: ", df['age'].mode()[0])

fig=px.histogram(data_frame=df, x='age', color='sex')
fig.show()

df.groupby('sex')['age'].value_counts()

df['sex'].value_counts()

male_count=726
Female_count=194
total_count=male_count+Female_count
percentage_male = (male_count / total_count) * 100
percentage_Female = (Female_count / total_count) * 100
print(f"The percentage of male is: {percentage_male:.2f}%")
print(f"The percentage of Female is: {percentage_Female:.2f}%")
Difference_percentage=(male_count-Female_count)/(Female_count)*100
print(f"percentage_male is{Difference_percentage: .2f}% more than female_percentage")

df['cp'].value_counts()

df.groupby('cp')['sex'].value_counts()

sns.countplot(df, x='cp', hue='sex')

fig=px.histogram(data_frame=df, x='age', color='cp')
fig.show()

df['trestbps'].describe()

df['trestbps'].unique()

df.isnull().sum().sort_values(ascending=False)
missing_data_cols=df.isnull().sum()[df.isnull().sum()>0].index.tolist()
missing_data_cols

df.head()

cat_cols=['sex','cp','fbs','restecg','exang','slope','thal']
bool_cols=['fbs','exang']
numeric_cols=['age','trestbps','chol','thalch','oldpeak','ca','num']

df[cat_cols] = df[cat_cols].astype(str)

for col in numeric_cols:
    df[col] = pd.to_numeric(df[col], errors='coerce')

print('Categorical Variables:',cat_cols)
print('Binary Variables:', bool_cols)
print('Numeric Variables:', numeric_cols)

dupes = df.columns[df.columns.duplicated()]
print("Duplicate column names:", dupes.tolist())

df.head()

def impute_categorical_missing_data(passed_col):

    df_null = df[df[passed_col].isnull()]
    df_not_null = df[df[passed_col].notnull()]

    X = df_not_null.drop(passed_col, axis=1)
    y = df_not_null[passed_col]

    other_missing_cols = [col for col in missing_data_cols if col != passed_col]

    label_encoder = LabelEncoder()

    for col in X.columns:
        if X[col].dtype == 'object' or X[col].dtype == 'category':
            X[col] = label_encoder.fit_transform(X[col])

    if passed_col in bool_cols:
        y = label_encoder.fit_transform(y)

    iterative_imputer = IterativeImputer(estimator=RandomForestRegressor(random_state=42), add_indicator=True)

    for col in other_missing_cols:
        if X[col].isnull().sum() > 0:
            col_with_missing_values = X[col].values.reshape(-1, 1)
            imputed_values = iterative_imputer.fit_transform(col_with_missing_values)
            X[col] = imputed_values[:, 0]
        else:
            pass

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    rf_classifier = RandomForestClassifier()

    rf_classifier.fit(X_train, y_train)

    y_pred = rf_classifier.predict(X_test)

    acc_score = accuracy_score(y_test, y_pred)

    print("The feature '"+ passed_col+ "' has been imputed with", round((acc_score * 100), 2), "accuracy\n")

    X = df_null.drop(passed_col, axis=1)

    for col in X.columns:
        if X[col].dtype == 'object' or X[col].dtype == 'category':
            X[col] = label_encoder.fit_transform(X[col])

    for col in other_missing_cols:
        if X[col].isnull().sum() > 0:
            col_with_missing_values = X[col].values.reshape(-1, 1)
            imputed_values = iterative_imputer.fit_transform(col_with_missing_values)
            X[col] = imputed_values[:, 0]
        else:
            pass

    if len(df_null) > 0:
        df_null[passed_col] = rf_classifier.predict(X)
        if passed_col in bool_cols:
            df_null[passed_col] = df_null[passed_col].map({0: False, 1: True})
        else:
            pass
    else:
        pass

    df_combined = pd.concat([df_not_null, df_null])

    return df_combined[passed_col]

def impute_continuous_missing_data(passed_col):

    df_null = df[df[passed_col].isnull()]
    df_not_null = df[df[passed_col].notnull()]

    X = df_not_null.drop(passed_col, axis=1)
    y = df_not_null[passed_col]

    other_missing_cols = [col for col in missing_data_cols if col != passed_col]

    label_encoder = LabelEncoder()

    for col in X.columns:
        if X[col].dtype == 'object' or X[col].dtype == 'category':
            X[col] = label_encoder.fit_transform(X[col])

    iterative_imputer = IterativeImputer(estimator=RandomForestRegressor(random_state=42), add_indicator=True)

    for col in other_missing_cols:
        if X[col].isnull().sum() > 0:
            col_with_missing_values = X[col].values.reshape(-1, 1)
            imputed_values = iterative_imputer.fit_transform(col_with_missing_values)
            X[col] = imputed_values[:, 0]
        else:
            pass

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    rf_regressor = RandomForestRegressor()

    rf_regressor.fit(X_train, y_train)

    y_pred = rf_regressor.predict(X_test)

    print("MAE =", mean_absolute_error(y_test, y_pred), "\n")
    print("RMSE =", mean_squared_error(y_test, y_pred), "\n")
    print("R2 =", r2_score(y_test, y_pred), "\n")

    X = df_null.drop(passed_col, axis=1)

    for col in X.columns:
        if X[col].dtype == 'object' or X[col].dtype == 'category':
            X[col] = label_encoder.fit_transform(X[col])

    for col in other_missing_cols:
        if X[col].isnull().sum() > 0:
            col_with_missing_values = X[col].values.reshape(-1, 1)
            imputed_values = iterative_imputer.fit_transform(col_with_missing_values)
            X[col] = imputed_values[:, 0]
        else:
            pass

    if len(df_null) > 0:
        df_null[passed_col] = rf_regressor.predict(X)
    else:
        pass

    df_not_null = df_not_null.reset_index(drop=True)
    df_null = df_null.reset_index(drop=True)

    df_combined = pd.concat([df_not_null, df_null])

    df_combined = df_combined.reset_index(drop=True)

    return df_combined[passed_col]

for cols in missing_data_cols:
    print('missing_values', cols, ':', str(round(df.isnull().sum()/len(df)*100, 2))+'%')
    if cols in cat_cols:
        df[cols] = impute_categorical_missing_data(cols)
    elif cols in numeric_cols:
        df[cols] = impute_continuous_missing_data(cols)
    else:
        pass

df.groupby('trestbps')['sex'].value_counts()

bins=[0,60,80,130, 135, 140]
labels=['very_low', 'low','Normal', 'high', 'very_high']
df['trestbps_bins']=pd.cut(df['trestbps'], bins=bins, labels=labels)
df.head()

df['trestbps_bins'].fillna(df['trestbps_bins'].mode()[0], inplace=True)
df['trestbps_bins'].isnull().sum()
df.head()

sns.histplot(df, x='trestbps', kde=True)

fig=px.histogram(df, x='trestbps', color='sex')
fig.show()

df['chol'].describe()

df.groupby('chol')['sex'].value_counts()

fig=px.histogram(data_frame=df, x='chol', color='sex')
fig.show()

df['fbs'].value_counts()

df.groupby('fbs')['sex'].value_counts()

df.groupby('fbs')['age'].value_counts()

fig = px.bar(
    data_frame=df,
    x='fbs',
    y='age',
    color='sex',
    color_discrete_sequence=['#1f3b4d', '#8b0000']  # Example: blue and red
)
fig.show()

df.head()

df['restecg'].value_counts()

df['restecg']=df['restecg'].apply(lambda x:x.replace(' ', '-')if ' ' in str(x) else x)

df.groupby('restecg')['sex'].value_counts()

fig=px.histogram(data_frame=df, x='age', color='restecg')
fig.show()

df['thalch'].describe()

fig=px.histogram(data_frame=df, x='thalch', color='sex')
fig.show()

df['oldpeak'].describe()

sns.barplot(df, y='oldpeak', hue='sex')

df['slope'].value_counts()

fig=px.histogram(data_frame=df, x='age', color='slope')
fig.show()

df['ca'].describe()

df['thal'].value_counts()

df['thal']=df['thal'].apply(lambda x:x.replace(' ', '-')if ' ' in str(x) else x)

df.groupby('thal')['sex'].value_counts()

fig=px.histogram(data_frame=df, x='sex', color='thal')
fig.show()

bins=[0,1,2,3,4]
labels=['No-Heart-Disease', 'Mild-Heart-Disease', 'Moderate-Heart-Disease', 'Severe-Heart-Disease']
df['num_bins']=pd.cut(df['num'], bins=bins, labels=labels)

df['num_bins'].fillna(df['num_bins'].mode()[0], inplace=True)

df['num'].value_counts()

df.groupby('num')['sex'].value_counts()

fig=px.box(data_frame=df, y='age')
fig.show()
fig=px.box(data_frame=df, y='trestbps')
fig.show()
fig=px.box(data_frame=df, y='chol')
fig.show()
fig=px.box(data_frame=df, y='thalch')
fig.show()
fig=px.box(data_frame=df, y='oldpeak')
fig.show()

df[df['trestbps']==0]
df=df[df['trestbps']!=0]

le_sex=LabelEncoder()
df['sex']=le_sex.fit_transform(df[['sex']])
le_cp=LabelEncoder()
df['cp']=le_cp.fit_transform(df[['cp']])
le_fbs=LabelEncoder()
df['fbs']=le_fbs.fit_transform(df[['fbs']])
le_restecg=LabelEncoder()
df['restecg']=le_restecg.fit_transform(df[['restecg']])
le_exang=LabelEncoder()
df['exang']=le_exang.fit_transform(df[['exang']])
le_slope=LabelEncoder()
df['slope']=le_slope.fit_transform(df[['slope']])
le_thal=LabelEncoder()
df['thal']=le_thal.fit_transform(df[['thal']])
le_trestbps_bins=LabelEncoder()
df['trestbps_bins']=le_trestbps_bins.fit_transform(df[['trestbps_bins']])
le_num_bins=LabelEncoder()
df['num_bins']=le_trestbps_bins.fit_transform(df[['num_bins']])
df.head()

cat_cols=['sex','cp','fbs','restecg','exang','slope','thal','trestbps_bins','num_bins']
bool_cols=['fbs','exang']
numeric_cols=['age','trestbps','chol','thalch','oldpeak','ca']

scalar=StandardScaler()
df[numeric_cols]=scalar.fit_transform(df[numeric_cols])

from sklearn.preprocessing import QuantileTransformer
qt_normal=QuantileTransformer(output_distribution='normal', random_state=42)
df[numeric_cols]=qt_normal.fit_transform(df[numeric_cols])

df.head()

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# X=df.drop('num', axis=1)
# y=df['num']
# random_state=42
# 
# X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=42,stratify=y)
# models = {
#     'Logistic Regression': (LogisticRegression(random_state=42), {}),
#     'KNN': (KNeighborsClassifier(), {'model__n_neighbors': [10, 30]}),
#     'SVC': (SVC(random_state=42), {'model__gamma': ['scale', 'auto']}),
#     'DecisionTreeClassifier': (DecisionTreeClassifier(random_state=42), {'model__max_depth': [5, 10, 15]}),
#     'RandomForestClassifier': (RandomForestClassifier(random_state=42), {'model__n_estimators': [100, 200, 300], 'model__max_depth': [5, 10, 15]}),
#     'GradientBoostingClassifier': (GradientBoostingClassifier(random_state=42), {'model__learning_rate': [0.1, 0.01, 0.001]}),
#     'AdaBoostClassifier': (AdaBoostClassifier(random_state=42), {'model__n_estimators': [50, 100, 200]}),
#     'XGBClassifier': (XGBClassifier(random_state=42), {'model__max_depth': [3, 4, 5]})
# }
# best_model=None
# best_accuracy=0
# for name,(models,params) in models.items():
#     pipeline=Pipeline(steps=[
#         ('model',models)
# ])
#     scores=cross_val_score(pipeline, X_train, y_train,cv=5, verbose=0)
#     mean_accuracy=scores.mean()
#     pipeline.fit(X_train,y_train)
#     y_pred=pipeline.predict(X_test)
#     accuracy=accuracy_score(y_test,y_pred)
#     f1=f1_score(y_test,y_pred,average='macro')
#     recall = recall_score(y_test, y_pred, average='macro')
#     precision = precision_score(y_test, y_pred, average='macro')
# 
#     if hasattr(pipeline, 'predict_proba'):
#         y_pred_proba = pipeline.predict_proba(X_test)
#         try:
#              roc_auc = roc_auc_score(y_test, y_pred_proba, multi_class='ovr')
#         except ValueError as e:
#              print(f"Could not calculate ROC AUC for {name}: {e}")
#              roc_auc = None
#     else:
#         print(f"Model {name} does not support predict_proba. Skipping ROC AUC calculation.")
#         roc_auc = None
# 
#     print('Model', name)
#     print('Cross_validation accuracy', mean_accuracy)
#     print('accuracy', accuracy)
#     print('f1_score', f1)
#     if roc_auc is not None:
#         print('roc_auc', roc_auc)
#     #print('recall', recall)
#     #print('precision', precision)
#     print('\n')
# 
# 
# 
#     if accuracy > best_accuracy:
#        best_accuracy = accuracy
#        best_model = pipeline
# print("Best Model is:", best_model)
# 
# 
# 
#

X= df.drop('num', axis=1)
y = df['num']

Label_Encoder = LabelEncoder()

for col in X.columns:
    if X[col].dtype == 'object' or X[col].dtype == 'category':
        X[col] = Label_Encoder.fit_transform(X[col])
    else:
        pass



X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

models = {
    ('Logistic Regression', LogisticRegression(random_state=42)),
    ('KNN', KNeighborsClassifier()),
    ('SVC', SVC(random_state=42)),
    ('DecisionTreeClassifier', DecisionTreeClassifier(random_state=42)),
    ('RandomForestClassifier', RandomForestClassifier(random_state=42)),
    ('GradientBoostingClassifier', GradientBoostingClassifier(random_state=42)),
    ('AdaBoostClassifier', AdaBoostClassifier(random_state=42)),
    ('XGBClassifier', XGBClassifier(random_state=42))
}

best_model = None
best_accuracy = 0.0

for name, model in models:
    pipeline = Pipeline([
        ('model', model)
    ])

    scores = cross_val_score(pipeline, X_train, y_train, cv=5)

    mean_accuracy = scores.mean()

    pipeline.fit(X_train, y_train)

    y_pred = pipeline.predict(X_test)

    accuracy = accuracy_score(y_test, y_pred)
    f1=f1_score(y_test,y_pred, average='macro')
    recall = recall_score(y_test, y_pred, average='macro')
    precision = precision_score(y_test, y_pred, average='macro')
    if hasattr(pipeline, 'predict_proba'):
        y_pred_proba = pipeline.predict_proba(X_test)
        try:
             roc_auc = roc_auc_score(y_test, y_pred_proba, multi_class='ovr')
        except ValueError as e:
             print(f"Could not calculate ROC AUC for {name}: {e}")
             roc_auc = None
    else:
        print(f"Model {name} does not support predict_proba. Skipping ROC AUC calculation.")
        roc_auc = None


    print("Model", name)
    print("Cross Validatino accuracy: ", mean_accuracy)
    print("Test Accuracy: ", accuracy)
    print('f1_score', f1)
    if roc_auc is not None:
        print('roc_auc', roc_auc)
    #print('recall', recall)
    #print('precision', precision)
    print('\n')
    print()

    if accuracy > best_accuracy:
        best_accuracy = accuracy
        best_model = pipeline

print("Best Model: ", best_model)

import pickle

with open('best_model.pkl', 'wb') as file:
    #pickle.dump(best_model, file)

from google.colab import files
files.download('best_model.pkl')